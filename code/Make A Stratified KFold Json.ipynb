{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './data/data/'\n",
    "with open(os.path.join(folder_path,'train_all.json'), 'r') as outfile:\n",
    "    ann = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'image_id': [],\n",
    "           'class_name': [],\n",
    "           'class_id': [],\n",
    "           'x_min': [],\n",
    "           'y_min': [],\n",
    "           'w': [],\n",
    "           'h': []}\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "classes = [\"UNKNOWN\", \"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \"Glass\",\n",
    "           \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\"]\n",
    "\n",
    "for ann_dict in ann['annotations']:\n",
    "    file_name = ann['images'][ann_dict['image_id']]['file_name']\n",
    "    row = [file_name, classes[ann_dict['category_id']], int(ann_dict['category_id']), \n",
    "           ann_dict['bbox'][0], ann_dict['bbox'][1], ann_dict['bbox'][2], ann_dict['bbox'][3]]\n",
    "    df = df.append(pd.Series(row, index=df.columns), ignore_index=True)\n",
    "df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    n_folds = 5\n",
    "    seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_group_k_fold(X, y, groups, k, seed=None):\n",
    "    labels_num = np.max(y) + 1\n",
    "    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "    y_distr = Counter()\n",
    "\n",
    "    for label, g in zip(y, groups):\n",
    "        y_counts_per_group[g][label] += 1\n",
    "        y_distr[label] += 1\n",
    "\n",
    "    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "    groups_per_fold = defaultdict(set)\n",
    "\n",
    "    def eval_y_counts_per_fold(y_counts, fold):\n",
    "        y_counts_per_fold[fold] += y_counts\n",
    "        std_per_label = []\n",
    "        for label in range(labels_num):\n",
    "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
    "            std_per_label.append(label_std)\n",
    "        y_counts_per_fold[fold] -= y_counts\n",
    "        return np.mean(std_per_label)\n",
    "\n",
    "    groups_and_y_counts = list(y_counts_per_group.items())\n",
    "    random.Random(seed).shuffle(groups_and_y_counts)\n",
    "\n",
    "    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for i in range(k):\n",
    "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "            if min_eval is None or fold_eval < min_eval:\n",
    "                min_eval = fold_eval\n",
    "                best_fold = i\n",
    "        y_counts_per_fold[best_fold] += y_counts\n",
    "        groups_per_fold[best_fold].add(g)\n",
    "\n",
    "    all_groups = set(groups)\n",
    "    for i in range(k):\n",
    "        train_groups = all_groups - groups_per_fold[i]\n",
    "        test_groups = groups_per_fold[i]\n",
    "\n",
    "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "        yield train_indices, test_indices\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Seed All\n",
    "\n",
    "    Args:\n",
    "        seed: seed number\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def get_folds(df, config):\n",
    "    df_folds = df[['image_id']].copy()\n",
    "    df_folds.loc[:, 'bbox_count'] = 1\n",
    "    df_folds = df_folds.groupby('image_id').count()\n",
    "    df_folds['fold'] = 0\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(\n",
    "            stratified_group_k_fold(df, df['class_id'], df['image_id'], config.n_folds, config.seed)):\n",
    "        trn_ids = df.loc[trn_idx, 'image_id'].unique()\n",
    "        val_ids = df.loc[val_idx, 'image_id'].unique()\n",
    "        assert len(set(trn_ids).intersection(set(val_ids))) == 0\n",
    "\n",
    "        df_folds.loc[val_ids, 'fold'] = fold\n",
    "    return df_folds\n",
    "\n",
    "\n",
    "def load_all_data():\n",
    "    meta_df = pd.read_csv(f\"data.csv\")\n",
    "    meta_df = meta_df.reset_index(drop=True)\n",
    "\n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = load_all_data()\n",
    "meta_df['class_id'] = meta_df['class_id'].astype(int)\n",
    "\n",
    "seed_everything()\n",
    "f_folds = get_folds(meta_df, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_folds = f_folds.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names = f_folds['image_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './data/data'\n",
    "\n",
    "anns_file_path = dataset_path + '/' + 'train.json'\n",
    "# Read annotations\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 1/5 [00:56<03:47, 56.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 2/5 [01:56<02:52, 57.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 3/5 [02:54<01:55, 57.70s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 4/5 [03:53<00:58, 58.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 5/5 [04:51<00:00, 58.31s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm(range(5)): \n",
    "    images = []\n",
    "    annotations = []\n",
    "    train_file_names = f_folds[f_folds['fold'] != idx]['image_id'].unique()\n",
    "    valid_file_names = f_folds[f_folds['fold'] == idx]['image_id'].unique()\n",
    "    for i, train_file_name in (enumerate(train_file_names)):\n",
    "        images.append(dict(\n",
    "                license=0,\n",
    "                url=None,\n",
    "                file_name=train_file_name,\n",
    "                height=512,\n",
    "                width=512,\n",
    "                date_captured=None,\n",
    "                id=i\n",
    "            ))\n",
    "\n",
    "        image_id = list(filter(lambda x: x['file_name'] == train_file_name, ann['images']))[0]['id']\n",
    "        for x in list(filter(lambda x: x['image_id'] == image_id, ann['annotations'])):\n",
    "            annotations.append(dict(id=len(annotations), \n",
    "                                image_id=i, \n",
    "                                category_id=x['category_id'], \n",
    "                                segmentation=x['segmentation'],\n",
    "                                area=x['area'], \n",
    "                                bbox=x['bbox'], \n",
    "                                iscrowd=x['iscrowd']))\n",
    "    \n",
    "    train_ann = {}\n",
    "    train_ann['images'] =  images\n",
    "    train_ann['annotations'] = annotations\n",
    "    train_ann['categories'] = categories\n",
    "    \n",
    "    with open(f'train_data{idx}.json', 'w') as f:\n",
    "        json.dump(train_ann, f, indent=4)\n",
    "        \n",
    "    images = []\n",
    "    annotations = []\n",
    "    for i, valid_file_name in (enumerate(valid_file_names)):\n",
    "        images.append(dict(\n",
    "                license=0,\n",
    "                url=None,\n",
    "                file_name=valid_file_name,\n",
    "                height=512,\n",
    "                width=512,\n",
    "                date_captured=None,\n",
    "                id=i\n",
    "            ))\n",
    "\n",
    "        image_id = list(filter(lambda x: x['file_name'] == valid_file_name, ann['images']))[0]['id']\n",
    "        for x in list(filter(lambda x: x['image_id'] == image_id, ann['annotations'])):\n",
    "            annotations.append(dict(id=len(annotations), \n",
    "                                image_id=i, \n",
    "                                category_id=x['category_id'], \n",
    "                                segmentation=x['segmentation'],\n",
    "                                area=x['area'], \n",
    "                                bbox=x['bbox'], \n",
    "                                iscrowd=x['iscrowd']))\n",
    "    \n",
    "    valid_ann = {}\n",
    "    valid_ann['images'] =  images\n",
    "    valid_ann['annotations'] = annotations\n",
    "    valid_ann['categories'] = categories\n",
    "        \n",
    "    with open(f'valid_data{idx}.json', 'w') as f:\n",
    "        json.dump(valid_ann, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
