{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1' # specify GPUs locally\n",
    "\n",
    "OUTPUT_DIR = './submission'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "dataset_path = './data/data'\n",
    "anns_file_path = dataset_path + '/' + 'train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import label_accuracy_score\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from adamp import AdamP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read annotations\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']\n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "nr_cats = len(categories)\n",
    "nr_annotations = len(anns)\n",
    "nr_images = len(imgs)\n",
    "\n",
    "# Load categories and super categories\n",
    "cat_names = []\n",
    "super_cat_names = []\n",
    "super_cat_ids = {}\n",
    "super_cat_last_name = ''\n",
    "nr_super_cats = 0\n",
    "for cat_it in categories:\n",
    "    cat_names.append(cat_it['name'])\n",
    "    super_cat_name = cat_it['supercategory']\n",
    "    # Adding new supercat\n",
    "    if super_cat_name != super_cat_last_name:\n",
    "        super_cat_names.append(super_cat_name)\n",
    "        super_cat_ids[super_cat_name] = nr_super_cats\n",
    "        super_cat_last_name = super_cat_name\n",
    "        nr_super_cats += 1\n",
    "        \n",
    "# Count annotations\n",
    "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
    "for ann in anns:\n",
    "    cat_histogram[ann['category_id']] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "df = df.sort_values('Number of annotations', 0, False)\n",
    "\n",
    "# category labeling \n",
    "sorted_temp_df = df.sort_index()\n",
    "\n",
    "# background = 0 에 해당되는 label 추가 후 기존들을 모두 label + 1 로 설정\n",
    "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
    "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = list(sorted_df.Categories)\n",
    "\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks.astype(np.float32)\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if CFG.apex:\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG  \n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug=False\n",
    "    img_size=512\n",
    "    max_len=275\n",
    "    print_freq=1000\n",
    "    num_workers=4\n",
    "    model_name='timm-efficientnet-b4' #['timm-efficientnet-b4', 'tf_efficientnet_b0_ns']\n",
    "    size=512 # [512, 1024]\n",
    "    freeze_epo = 0\n",
    "    warmup_epo = 1\n",
    "    cosine_epo = 39 #14 #19\n",
    "    warmup_factor=10\n",
    "    scheduler='GradualWarmupSchedulerV2' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts', 'GradualWarmupSchedulerV2', 'get_linear_schedule_with_warmup']\n",
    "    epochs=freeze_epo + warmup_epo + cosine_epo # not to exceed 9h #[1, 5, 10]\n",
    "    factor=0.2 # ReduceLROnPlateau\n",
    "    patience=4 # ReduceLROnPlateau\n",
    "    eps=1e-6 # ReduceLROnPlateau\n",
    "    T_max=4 # CosineAnnealingLR\n",
    "    T_0=4 # CosineAnnealingWarmRestarts\n",
    "    encoder_lr=3e-5 #[1e-4, 3e-5]\n",
    "    min_lr=1e-6\n",
    "    batch_size=32 + 0 #[64, 256 + 128, 512, 1024, 512 + 256 + 128, 2048]\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=5\n",
    "    dropout=0.5\n",
    "    seed=42\n",
    "    smoothing=0.05\n",
    "    n_fold=5\n",
    "    trn_fold=[0]\n",
    "    trn_fold=[0, 1, 2, 3, 4] # [0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    apex=False\n",
    "    log_day='0504'\n",
    "    model_type=model_name\n",
    "    version='v1-1'\n",
    "    load_state=False\n",
    "    cutmix=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "#sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose, Blur, GaussNoise, MotionBlur, MedianBlur, OpticalDistortion, ElasticTransform, \n",
    "    GridDistortion, IAAPiecewiseAffine, CLAHE, IAASharpen, IAAEmboss, HueSaturationValue, ToGray, JpegCompression\n",
    "    )\n",
    "\n",
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "            A.Rotate(p=.25, limit=(-30, 30)), \n",
    "            A.OneOf([\n",
    "                A.HorizontalFlip(p=.5), \n",
    "                A.VerticalFlip(p=.5),\n",
    "            ], p=1), \n",
    "            A.Cutout(num_holes=10, \n",
    "                        max_h_size=int(.1 * CFG.img_size), max_w_size=int(.1 * CFG.img_size), \n",
    "                        p=.25),\n",
    "            A.ShiftScaleRotate(p=.25),\n",
    "            # A.CLAHE(p=.25), \n",
    "            A.RandomResizedCrop(CFG.size, CFG.size, scale = [0.75, 1], p=1),\n",
    "            A.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225)\n",
    "            ),\n",
    "            ToTensorV2(transpose_mask=False)\n",
    "        ])\n",
    "    \n",
    "val_transform = A.Compose([\n",
    "                            A.Normalize(\n",
    "                                mean=(0.485, 0.456, 0.406),\n",
    "                                std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "                            ),                           \n",
    "                            ToTensorV2(transpose_mask=False)\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Normalize(\n",
    "                                mean=(0.485, 0.456, 0.406),\n",
    "                                std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "                            ),    \n",
    "                    ToTensorV2(transpose_mask=False)\n",
    "        ])\n",
    "\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=CFG.batch_size,\n",
    "                                           num_workers=CFG.num_workers, \n",
    "                                           pin_memory=True,\n",
    "                                           drop_last=True, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# v13. drop-last False \n",
    "valid_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=CFG.batch_size,\n",
    "                                         num_workers=CFG.num_workers, \n",
    "                                         pin_memory=True,\n",
    "                                         # drop_last=True, \n",
    "                                         shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=CFG.batch_size,\n",
    "                                          num_workers=CFG.num_workers,\n",
    "                                          pin_memory=True,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='timm-efficientnet-b4', pretrained=False):\n",
    "        super().__init__()        \n",
    "        self.encoder = smp.FPN(encoder_name=model_name, encoder_weights=\"noisy-student\", classes=12) # [imagenet, noisy-student]\n",
    "    \n",
    "    #@autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for fold in range(5): \n",
    "    model_path = f'./submissiond{CFG.dropout}_s{CFG.seed}_{CFG.model_name}_v1-1_fold{fold}_best.pth'\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = Encoder(CFG.model_name, pretrained=False)\n",
    "    model.load_state_dict(checkpoint['encoder'])\n",
    "    models += [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 이미지 출력해서 잘 나온 값들 살펴보기 \n",
    "best = pd.read_csv(\"./submission/best.csv\") # lb가 가장 좋았던 파일 (256, 256 size submission)\n",
    "\n",
    "COLORS =[\n",
    "        [0, 0, 0], # 검정 - 배경\n",
    "        [129, 236, 236], # 청록 # UNKNOWN \n",
    "        [2, 132, 227], # 파랑 # 일반쓰레기 \n",
    "        [232, 67, 147], # 진한분홍 # 종이 \n",
    "        [255, 234, 267],# 연분홍 # 종이팩 \n",
    "        [0, 184, 148], # 뚱한녹색 # 메탈 \n",
    "        [85, 239, 196], # 밝은파스칼청록 # 유리 \n",
    "        [48, 51, 107], # 남색~보라 # 플라스틱 \n",
    "        [255, 159, 26], # 주황 # 스트로폼 \n",
    "        [255, 204, 204], #연분홍 # 플라스틱 가방 \n",
    "        [179, 57, 57], # 적갈색 # 배터리 \n",
    "        [248, 243, 212], # 밝은 노랑 # 옷 \n",
    "    ]\n",
    "\n",
    "COLORS = np.vstack([[0, 0, 0], COLORS]).astype('uint8')\n",
    "\n",
    "dataset_path = './data/data'\n",
    "for i in range(0, best.shape[0]): \n",
    "    try: \n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 16))\n",
    "        images = cv2.imread(os.path.join(dataset_path, best.loc[i, 'image_id']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32) / 255\n",
    "\n",
    "        masks = np.array(list(map(int, re.findall(\"\\d+\", best.loc[i]['PredictionString'])))).reshape(256, 256)\n",
    "        # Original image\n",
    "        ax1.imshow(images)\n",
    "        ax1.grid(False)\n",
    "        ax1.set_title(\"Original image : {}\".format(best.loc[i, 'image_id']), fontsize = 15)\n",
    "\n",
    "        # Predicted\n",
    "        ax2.imshow(COLORS[masks])\n",
    "        ax2.grid(False)\n",
    "        ax2.set_title(\"Unique values, category of transformed mask : {} \\n\".format([{int(k),category_names[int(k)]} for k in list(np.unique(masks))]), fontsize = 15)\n",
    "        plt.show()\n",
    "    except: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './data/data'\n",
    "for i in range(0, best.shape[0]): \n",
    "    try: \n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 16))\n",
    "        images = cv2.imread(os.path.join(dataset_path, best.loc[i, 'image_id']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32) / 255\n",
    "\n",
    "        masks = np.array(list(map(int, re.findall(\"\\d+\", best.loc[i]['PredictionString'])))).reshape(256, 256)\n",
    "        # Original image\n",
    "        ax1.imshow(images)\n",
    "        ax1.grid(False)\n",
    "        ax1.set_title(\"Original image : {}\".format(best.loc[i, 'image_id']), fontsize = 15)\n",
    "\n",
    "        # Predicted\n",
    "        ax2.imshow(COLORS[masks])\n",
    "        ax2.grid(False)\n",
    "        ax2.set_title(\"Unique values, category of transformed mask : {} \\n\".format([{int(k),category_names[int(k)]} for k in list(np.unique(masks))]), fontsize = 15)\n",
    "        plt.show()\n",
    "    except: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def close_contour(contour):\n",
    "    if not np.array_equal(contour[0], contour[-1]):\n",
    "        contour = np.vstack((contour, contour[0]))\n",
    "    return contour\n",
    "\n",
    "\n",
    "def binary_mask_to_polygon(binary_mask, tolerance=0):\n",
    "    \"\"\"Converts a binary mask to COCO polygon representation\n",
    "    Args:\n",
    "        binary_mask: a 2D binary numpy array where '1's represent the object\n",
    "        tolerance: Maximum distance from original points of polygon to approximated\n",
    "            polygonal chain. If tolerance is 0, the original coordinate array is returned.\n",
    "    \"\"\"\n",
    "    polygons = []\n",
    "    # pad mask to close contours of shapes which start and end at an edge\n",
    "    padded_binary_mask = np.pad(binary_mask, pad_width=1, mode='constant', constant_values=0)\n",
    "    contours = measure.find_contours(padded_binary_mask, 0.5)\n",
    "    contours = np.subtract(contours, 1)\n",
    "    for contour in contours:\n",
    "        contour = close_contour(contour)\n",
    "        contour = measure.approximate_polygon(contour, tolerance)\n",
    "        if len(contour) < 3:\n",
    "            continue\n",
    "        contour = np.flip(contour, axis=1)\n",
    "        segmentation = contour.ravel().tolist()\n",
    "        # after padding and subtracting 1 we may get -0.5 points in our segmentation \n",
    "        segmentation = [int(0) if i < 0 else int(i) for i in segmentation]\n",
    "        polygons.append(segmentation)\n",
    "\n",
    "    return polygons\n",
    "\n",
    "# binary_mask_to_polygon의 \n",
    "# temp_ann = binary_mask_to_polygon(augmented['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3271, 26401\n",
    "fold_path = dataset_path + '/' + 'train_data0.json'\n",
    "\n",
    "# Read annotations\n",
    "with open(fold_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "    \n",
    "images = dataset['images']\n",
    "annotations = dataset['annotations']\n",
    "categories = dataset['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesudo = pd.read_csv(\"./submission/Pesudo.csv\")\n",
    "best_submission = pd.read_csv(\"./submission/Best.csv\") # 이제까지 LB 성능이 가장 좋았던 파일 - 256, 256 으로 줄이지 않고 512, 512 유지한 상태 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import notebook, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import notebook, tqdm_notebook\n",
    "for fold in tqdm(range(5)): \n",
    "    fold_path = dataset_path + '/' + f'train_data{fold}.json'\n",
    "\n",
    "    # Read annotations\n",
    "    with open(fold_path, 'r') as f:\n",
    "        dataset = json.loads(f.read())\n",
    "\n",
    "    images = dataset['images']\n",
    "    annotations = dataset['annotations']\n",
    "    categories = dataset['categories']\n",
    "    \n",
    "    image_dict_id = images[-1]['id']\n",
    "    annotation_dict_id = annotations[-1]['id'] + 1\n",
    "    for id_ in tqdm_notebook(range(pesudo.shape[0])):\n",
    "        image_dict_id += 1\n",
    "        images_dict = {}\n",
    "        images_dict['license'] = 0\n",
    "        images_dict['url'] = None\n",
    "        images_dict['file_name'] = pesudo.loc[id_, 'image_id']\n",
    "        images_dict['height'] = 512\n",
    "        images_dict['width'] = 512\n",
    "        images_dict['date_captured'] = None\n",
    "        images_dict['id'] = image_dict_id\n",
    "        images += [images_dict]\n",
    "        for i in range(1, 11): \n",
    "            pesudo_dict = {}\n",
    "\n",
    "            A = np.zeros((512, 512))\n",
    "            mask = np.array(list(map(int, re.findall(\"\\d+\", best_submission[best_submission['image_id'] == pesudo.loc[id_, 'image_id']]['PredictionString'].values[0])))).reshape(512, 512)\n",
    "            x, y = np.where(mask==i)\n",
    "\n",
    "            L = []\n",
    "            for x_, y_ in zip(x, y): \n",
    "                L += [(x_, y_)]\n",
    "\n",
    "            if len(L) != 0: \n",
    "                idx = np.r_[L].T\n",
    "                A[idx[0], idx[1]] = 1\n",
    "                annotation_dict_id += 1\n",
    "                pesudo_dict['id'] = annotation_dict_id\n",
    "                pesudo_dict['image_id'] = image_dict_id\n",
    "                pesudo_dict['category_id'] = i\n",
    "                pesudo_dict['segmentation'] = binary_mask_to_polygon(A)\n",
    "                pesudo_dict['area'] = 0\n",
    "                pesudo_dict['bbox'] = [0, 0, 0, 0]\n",
    "                pesudo_dict['iscrowd'] = 0\n",
    "                annotations += [pesudo_dict]\n",
    "\n",
    "    train_ann = {}\n",
    "    train_ann['images'] =  images\n",
    "    train_ann['annotations'] = annotations\n",
    "    train_ann['categories'] = categories\n",
    "\n",
    "    with open(f'train_data_pesudo{fold}.json', 'w') as f:\n",
    "        json.dump(train_ann, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
