{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "- Starter using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1' # specify GPUs locally\n",
    "\n",
    "OUTPUT_DIR = './submission'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "dataset_path = './data/data'\n",
    "anns_file_path = dataset_path + '/' + 'train.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import label_accuracy_score\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read annotations\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']\n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "nr_cats = len(categories)\n",
    "nr_annotations = len(anns)\n",
    "nr_images = len(imgs)\n",
    "\n",
    "# Load categories and super categories\n",
    "cat_names = []\n",
    "super_cat_names = []\n",
    "super_cat_ids = {}\n",
    "super_cat_last_name = ''\n",
    "nr_super_cats = 0\n",
    "for cat_it in categories:\n",
    "    cat_names.append(cat_it['name'])\n",
    "    super_cat_name = cat_it['supercategory']\n",
    "    # Adding new supercat\n",
    "    if super_cat_name != super_cat_last_name:\n",
    "        super_cat_names.append(super_cat_name)\n",
    "        super_cat_ids[super_cat_name] = nr_super_cats\n",
    "        super_cat_last_name = super_cat_name\n",
    "        nr_super_cats += 1\n",
    "        \n",
    "# Count annotations\n",
    "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
    "for ann in anns:\n",
    "    cat_histogram[ann['category_id']] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "df = df.sort_values('Number of annotations', 0, False)\n",
    "\n",
    "# category labeling \n",
    "sorted_temp_df = df.sort_index()\n",
    "\n",
    "# background = 0 에 해당되는 label 추가 후 기존들을 모두 label + 1 로 설정\n",
    "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
    "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = list(sorted_df.Categories)\n",
    "\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks.astype(np.float32)\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG  \n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug=False\n",
    "    img_size=512\n",
    "    max_len=275\n",
    "    print_freq=1000\n",
    "    num_workers=4\n",
    "    model_name='timm-efficientnet-b5' #['timm-efficientnet-b4', 'tf_efficientnet_b0_ns']\n",
    "    size=512 # [512, 1024]\n",
    "    freeze_epo = 0\n",
    "    warmup_epo = 1\n",
    "    cosine_epo = 39 #14 #19\n",
    "    warmup_factor=10\n",
    "    scheduler='GradualWarmupSchedulerV2' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts', 'GradualWarmupSchedulerV2', 'get_linear_schedule_with_warmup']\n",
    "    epochs=freeze_epo + warmup_epo + cosine_epo # not to exceed 9h #[1, 5, 10]\n",
    "    factor=0.2 # ReduceLROnPlateau\n",
    "    patience=4 # ReduceLROnPlateau\n",
    "    eps=1e-6 # ReduceLROnPlateau\n",
    "    T_max=4 # CosineAnnealingLR\n",
    "    T_0=4 # CosineAnnealingWarmRestarts\n",
    "    encoder_lr=3e-5 #[1e-4, 3e-5]\n",
    "    min_lr=1e-6\n",
    "    batch_size=24 + 0 #[64, 256 + 128, 512, 1024, 512 + 256 + 128, 2048]\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=5\n",
    "    dropout=0.5\n",
    "    seed=42\n",
    "    smoothing=0.05\n",
    "    n_fold=5\n",
    "    trn_fold=[0]\n",
    "    trn_fold=[0, 1, 2, 3, 4] # [0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    apex=False\n",
    "    log_day='0505'\n",
    "    model_type=model_name\n",
    "    version='v1-1'\n",
    "    load_state=False\n",
    "    cutmix=False\n",
    "    pesudo=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if CFG.apex:\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    train = train.sample(n=2, random_state=CFG.seed).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "#sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 정의 및 DataLoader 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose, Blur, GaussNoise, MotionBlur, MedianBlur, OpticalDistortion, ElasticTransform, \n",
    "    GridDistortion, IAAPiecewiseAffine, CLAHE, IAASharpen, IAAEmboss, HueSaturationValue, ToGray, JpegCompression\n",
    "    )\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "            A.VerticalFlip(p=.25),\n",
    "            A.Cutout(num_holes=10, \n",
    "                        max_h_size=int(.1 * CFG.img_size), max_w_size=int(.1 * CFG.img_size), \n",
    "                        p=.25),\n",
    "            A.ShiftScaleRotate(p=.25),\n",
    "            A.RandomResizedCrop(CFG.size, CFG.size, scale = [0.75, 1], p=1),\n",
    "            A.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225)\n",
    "            ),\n",
    "            ToTensorV2(transpose_mask=False)\n",
    "        ])\n",
    "    \n",
    "val_transform = A.Compose([\n",
    "                            A.Normalize(\n",
    "                                mean=(0.485, 0.456, 0.406),\n",
    "                                std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "                            ),                           \n",
    "                            ToTensorV2(transpose_mask=False)\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Normalize(\n",
    "                                mean=(0.485, 0.456, 0.406),\n",
    "                                std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "                            ),    \n",
    "                    ToTensorV2(transpose_mask=False)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='timm-efficientnet-b4', pretrained=False):\n",
    "        super().__init__()        \n",
    "        self.encoder = smp.FPN(encoder_name=model_name, encoder_weights=\"noisy-student\", classes=12) # [imagenet, noisy-student]\n",
    "    \n",
    "    #@autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)\n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "    \n",
    "    \n",
    "class DiceBCELoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).mean()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  \n",
    "        Dice_BCE = 0.9*BCE + 0.1*dice_loss\n",
    "        \n",
    "        return Dice_BCE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(n_class * label_true[mask].astype(int) + label_pred[mask],\n",
    "                        minlength=n_class ** 2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def label_accuracy_score(hist):\n",
    "    \"\"\"\n",
    "    Returns accuracy score evaluation result.\n",
    "      - [acc]: overall accuracy\n",
    "      - [acc_cls]: mean accuracy\n",
    "      - [mean_iu]: mean IU\n",
    "      - [fwavacc]: fwavacc\n",
    "    \"\"\"\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "    acc_cls = np.nanmean(acc_cls)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iu = np.nanmean(iu)\n",
    "\n",
    "    freq = hist.sum(axis=1) / hist.sum()\n",
    "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "    return acc, acc_cls, mean_iu, iu, fwavacc\n",
    "\n",
    "\n",
    "def add_hist(hist, label_trues, label_preds, n_class):\n",
    "    \"\"\"\n",
    "        stack hist(confusion matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(train_loader, encoder, criterion, \n",
    "             optimizer, epoch,\n",
    "             scheduler, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    miou_score = AverageMeter()\n",
    "    # switch to train mode\n",
    "    encoder.train()\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    hist = np.zeros((12, 12))\n",
    "    for step, (images, targets) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        #images = torch.stack(images)       # (batch, channel, height, width)\n",
    "        #targets = torch.stack(targets).long()        # (batch, channel, height, width)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device).long()   \n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        if CFG.cutmix:\n",
    "            # generate mixed sample\n",
    "            lam = np.random.beta(1., 1.)\n",
    "            rand_index = torch.randperm(batch_size).cuda()\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "            images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            targets[:, bbx1:bbx2, bby1:bby2] = targets[rand_index, bbx1:bbx2, bby1:bby2]\n",
    "        \n",
    "        # =========================\n",
    "        # zero_grad()\n",
    "        # =========================\n",
    "        optimizer.zero_grad()\n",
    "        if CFG.apex:\n",
    "            with autocast():\n",
    "                y_preds = encoder(images)\n",
    "                loss = criterion(y_preds, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "        else:\n",
    "            y_preds = encoder(images)\n",
    "            loss = criterion(y_preds, targets)\n",
    "            loss.backward()\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        #loss.backward()\n",
    "        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            if CFG.apex:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            global_step += 1\n",
    "            \n",
    "        # record dice_coeff\n",
    "        y_preds = torch.argmax(y_preds.squeeze(), dim=1).detach().cpu().numpy()\n",
    "        hist = add_hist(hist, targets.detach().cpu().numpy(), y_preds, n_class=12)\n",
    "        acc, acc_cls, mIoU, iu, fwavacc = label_accuracy_score(hist)\n",
    "        miou_score.update(mIoU, batch_size)\n",
    "                \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'MioU: {miou.val:.4f}({miou.avg:.4f}) '\n",
    "                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n",
    "                  'Encoder LR: {encoder_lr:.6f}  '\n",
    "                  .format(\n",
    "                   epoch+1, step, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, miou=miou_score,\n",
    "                   remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                   encoder_grad_norm=encoder_grad_norm,\n",
    "                   encoder_lr=scheduler.get_lr()[0],\n",
    "                   ))\n",
    "    acc, acc_cls, mIoU, iu, fwavacc = label_accuracy_score(hist)\n",
    "    return losses.avg, mIoU\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, encoder, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    miou_score = AverageMeter()\n",
    "    \n",
    "    # switch to evaluation mode\n",
    "    encoder.eval()\n",
    "    #trues = []\n",
    "    #preds = []\n",
    "    start = end = time.time()\n",
    "    hist = np.zeros((12, 12))\n",
    "    for step, (images, targets) in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        # images = torch.stack(images)       # (batch, channel, height, width)\n",
    "        # targets = torch.stack(targets).long()   # (batch, channel, height, width)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device).long() \n",
    "        batch_size = images.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = encoder(images)\n",
    "        \n",
    "        loss = criterion(y_preds, targets)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        # record dice_coeff\n",
    "        y_preds = torch.argmax(y_preds.squeeze(), dim=1).detach().cpu().numpy()\n",
    "        hist = add_hist(hist, targets.detach().cpu().numpy(), y_preds, n_class=12)\n",
    "        acc, acc_cls, mIoU, iu, fwavacc = label_accuracy_score(hist)\n",
    "        miou_score.update(mIoU, batch_size)\n",
    "        #trues.append(labels.to('cpu').numpy())\n",
    "        #preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'MioU: {miou.val:.4f}({miou.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, miou=miou_score,\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "    #preds = np.concatenate(preds)\n",
    "    acc, acc_cls, mIoU, iu, fwavacc = label_accuracy_score(hist)\n",
    "    print(iu)\n",
    "    return losses.avg, mIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from segmentation_models.losses import bce_jaccard_loss\n",
    "\n",
    "from losses.soft_ce import SoftCrossEntropyLoss\n",
    "from losses.lovasz import LovaszLoss\n",
    "from utils import rand_bbox, copyblob, FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SoftCrossEntropyLoss(smooth_factor=CFG.smoothing, ignore_index=1) #['SoftCrossEntropyLoss(smooth_factor=CFG.smoothing, ignore_index=1)', DiceBCELoss()', 'DiceLoss()', 'nn.BCEWithLogitsLoss()']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(fold):\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    # train.json / validation.json / test.json 디렉토리 설정\n",
    "    if CFG.pesudo:\n",
    "        train_path = dataset_path + f'/train_data_pesudo{fold}.json'\n",
    "    else:\n",
    "        train_path = dataset_path + f'/train_data{fold}.json'\n",
    "    val_path = dataset_path + f'/valid_data{fold}.json'\n",
    "\n",
    "    # train dataset\n",
    "    train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "    # validation dataset\n",
    "    val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n",
    "\n",
    "\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=CFG.batch_size,\n",
    "                                               num_workers=CFG.num_workers, \n",
    "                                               pin_memory=True,\n",
    "                                               drop_last=True, \n",
    "                                               shuffle=True)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                             batch_size=CFG.batch_size,\n",
    "                                             num_workers=CFG.num_workers, \n",
    "                                             pin_memory=True,\n",
    "                                             # drop_last=True, \n",
    "                                             shuffle=False)\n",
    "\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='GradualWarmupSchedulerV2':\n",
    "            scheduler_cosine=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, CFG.cosine_epo)\n",
    "            scheduler_warmup=GradualWarmupSchedulerV2(optimizer, multiplier=CFG.warmup_factor, total_epoch=CFG.warmup_epo, after_scheduler=scheduler_cosine)\n",
    "            scheduler=scheduler_warmup        \n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    encoder = Encoder(CFG.model_name, pretrained=True)\n",
    "    encoder.to(device)\n",
    "    \n",
    "    if len(os.environ['CUDA_VISIBLE_DEVICES'].split(',')) > 1:\n",
    "        #print('DataParallel')\n",
    "        encoder = nn.DataParallel(encoder)\n",
    "\n",
    "    optimizer = Adam(encoder.parameters(), lr=CFG.encoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    # Log the network weight histograms (optional)\n",
    "    #wandb.watch(encoder, log='all')\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = SoftCrossEntropyLoss(smooth_factor=CFG.smoothing, ignore_index=1) #['SoftCrossEntropyLoss(smooth_factor=CFG.smoothing, ignore_index=1)', DiceBCELoss()', 'DiceLoss()', 'nn.BCEWithLogitsLoss()']\n",
    "    # criterion = FocalLoss()\n",
    "    best_score = 0\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss, avg_tr_miou = train_fn(train_loader, encoder, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, avg_val_miou = valid_fn(valid_loader, encoder, criterion, device)\n",
    "\n",
    "        # scoring\n",
    "        #score = get_score(valid_labels, text_preds)\n",
    "        score = avg_val_miou\n",
    "        \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(score)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, GradualWarmupSchedulerV2):\n",
    "            scheduler.step(epoch)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {avg_val_miou:.4f}')\n",
    "        \n",
    "   \n",
    "        wandb.log({\n",
    "            \"avg_loss\": avg_loss,\n",
    "            \"avg_val_loss\": avg_val_loss,\n",
    "            \"Score\": score,\n",
    "            'epoch': epoch,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        })\n",
    "\n",
    "        \n",
    "        model_to_save = encoder.module if hasattr(encoder, 'module') else encoder\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'encoder': model_to_save.state_dict(), \n",
    "                        'optimizer': optimizer.state_dict(), \n",
    "                        'scheduler': scheduler.state_dict(), \n",
    "                        #'text_preds': text_preds,\n",
    "                       },\n",
    "                        OUTPUT_DIR+f'{CFG.log_day}_d{CFG.dropout}_s{CFG.seed}_{CFG.model_name}_{CFG.version}_fold{fold}_best.pth')\n",
    "            best_oof = avg_val_miou\n",
    "            # print(best_oof)\n",
    "    # return best_oof #text_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main(rank=0, world_size=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare: 1.train  2.folds\n",
    "    \"\"\"\n",
    "    #rank = 2\n",
    "    #world_size = 2\n",
    "    if CFG.train:\n",
    "        # train\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                \n",
    "                # train        \n",
    "                seed_torch(seed=CFG.seed)\n",
    "                wandb.init(project='Trash-Segmentation', name=f'{CFG.log_day}_{CFG.model_type}+FPN+AUG_{CFG.version}_fold{fold}', entity='choco_9966')\n",
    "                config = wandb.config          # Initialize config\n",
    "\n",
    "                config.batch_size = CFG.batch_size\n",
    "                config.encoder_lr = CFG.encoder_lr\n",
    "                config.seed = CFG.seed\n",
    "                config.weight_decay = CFG.weight_decay\n",
    "                config.gradient_accumulation_steps = CFG.gradient_accumulation_steps\n",
    "                config.scheduler = CFG.scheduler\n",
    "                config.model_name = CFG.model_name\n",
    "                config.apex = CFG.apex\n",
    "                config.num_workers = CFG.num_workers\n",
    "                config.img_size = CFG.size\n",
    "                config.print_freq = CFG.print_freq\n",
    "                config.n_fold = CFG.n_fold\n",
    "                config.train = CFG.train\n",
    "                config.epochs = CFG.epochs\n",
    "        #                 config.inference = CFG.inference\n",
    "        #                 config.swa = CFG.swa\n",
    "        #                 config.swa_start = CFG.swa_start\n",
    "        #                 config.swa_lr = CFG.swa_lr\n",
    "        #                 config.swa = CFG.swa\n",
    "                config.smoothing = CFG.smoothing\n",
    "\n",
    "                train_loop(fold)\n",
    "                wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(test_loader):\n",
    "\n",
    "            # inference (512 x 512)\n",
    "            for n, model in enumerate(models): \n",
    "                model = model.to(device)\n",
    "                model.eval()\n",
    "                if n == 0: \n",
    "                    outs = model(torch.stack(imgs).to(device))\n",
    "                else: \n",
    "                    outs += model(torch.stack(imgs).to(device))\n",
    "            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "\n",
    "            oms = np.array(temp_mask)\n",
    "            oms = np.around(oms.reshape([oms.shape[0], size*size])).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "            \n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = dataset_path + f'/test.json'\n",
    "\n",
    "# test dataset\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=CFG.batch_size,\n",
    "                                          num_workers=CFG.num_workers,\n",
    "                                          pin_memory=True,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='timm-efficientnet-b4', pretrained=False):\n",
    "        super().__init__()        \n",
    "        self.encoder = smp.FPN(encoder_name=model_name, encoder_weights=\"noisy-student\", classes=12) # [imagenet, noisy-student]\n",
    "    \n",
    "    #@autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "# 추론을 실행하기 전에는 반드시 설정 (batch normalization, dropout 를 평가 모드로 설정)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for fold in range(5): \n",
    "    model_path = f'./submission{CFG.log_day}_d{CFG.dropout}_s{CFG.seed}_{CFG.model_name}_{CFG.version}_fold{fold}_best.pth'\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = Encoder(CFG.model_name, pretrained=False)\n",
    "    model.load_state_dict(checkpoint['encoder'])\n",
    "    models += [model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Scale TTA (No ttach package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import scipy\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydensecrf.densecrf as dcrf\n",
    "from pydensecrf.utils import unary_from_softmax, create_pairwise_bilateral\n",
    "\n",
    "'''\n",
    "# Default Values are\n",
    "apperance_kernel = [8, 164, 100] # PairwiseBilateral [sxy, srgb, compat]  \n",
    "spatial_kernel = [3, 10]         # PairwiseGaussian  [sxy, compat] \n",
    "\n",
    "# or if you want to to specify seprately for each XY direction and RGB color channel then\n",
    "\n",
    "apperance_kernel = [(1.5, 1.5), (64, 64, 64), 100] # PairwiseBilateral [sxy, srgb, compat]  \n",
    "spatial_kernel = [(0.5, 0.5), 10]                  # PairwiseGaussian  [sxy, compat] \n",
    "'''\n",
    "# https://www.programcreek.com/python/example/106424/pydensecrf.densecrf.DenseCRF2D\n",
    "h, w = 512, 512\n",
    "def dense_crf(probs, img=None, n_classes=12, n_iters=10, scale_factor=1):\n",
    "    c,h,w = probs.shape\n",
    "    \n",
    "    if img is not None:\n",
    "        assert(img.shape[1:3] == (h, w))\n",
    "        img = np.transpose(img,(1,2,0)).copy(order='C')\n",
    "        img = np.uint8(255 * img)\n",
    "\n",
    "    d = dcrf.DenseCRF2D(w, h, n_classes) # Define DenseCRF model.\n",
    "\n",
    "    unary = unary_from_softmax(probs)\n",
    "    unary = np.ascontiguousarray(unary)\n",
    "    d.setUnaryEnergy(unary)\n",
    "    d.addPairwiseGaussian(sxy=(3,3), compat=10)\n",
    "    d.addPairwiseBilateral(sxy=10, srgb=5, rgbim=np.copy(img), compat=10)\n",
    "    Q = d.inference(n_iters)\n",
    "\n",
    "    preds = np.array(Q, dtype=np.float32).reshape((n_classes, h, w))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [0.75, 1.0, 1.25] \n",
    "def multi_scale_predict(model, image, scales, num_classes, device, flip=False):\n",
    "    input_size = (image.size(2), image.size(3))\n",
    "    upsample = nn.Upsample(size=input_size, mode='bilinear', align_corners=True)\n",
    "    total_predictions = np.zeros((image.size(0), num_classes, image.size(2), image.size(3)))\n",
    "\n",
    "    image = image.data.data.cpu().numpy()\n",
    "    for scale in scales:\n",
    "        scaled_img = ndimage.zoom(image, (1.0, 1.0, float(scale), float(scale)), order=1, prefilter=False)\n",
    "        scaled_img = torch.from_numpy(scaled_img).to(device)\n",
    "        scaled_prediction = upsample(model(scaled_img).cpu())\n",
    "\n",
    "        if flip:\n",
    "            fliped_img = scaled_img.flip(-1).to(device)\n",
    "            fliped_predictions = upsample(model(fliped_img).cpu())\n",
    "            scaled_prediction = 0.5 * (fliped_predictions.flip(-1) + scaled_prediction)\n",
    "        total_predictions += scaled_prediction.data.cpu().numpy()\n",
    "\n",
    "    total_predictions /= len(scales)\n",
    "    return total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ]),\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(test_loader):\n",
    "\n",
    "            # inference (512 x 512)\n",
    "            for n, model in enumerate(models): \n",
    "                model = model.to(device)\n",
    "                model.eval()\n",
    "                if n == 0: \n",
    "                    outs = multi_scale_predict(model, torch.stack(imgs).to(device), scales, 12, device, flip=True)\n",
    "                else: \n",
    "                    outs += multi_scale_predict(model, torch.stack(imgs).to(device), scales, 12, device, flip=True)\n",
    "\n",
    "                    \n",
    "            probs_array = []\n",
    "            for image, prob in zip(imgs, outs):\n",
    "                prob = F.softmax(torch.from_numpy(prob), dim=0)                    \n",
    "                prob = dense_crf(img=np.around(invTrans(image).cpu().numpy()).astype(float), probs=prob.cpu().numpy())\n",
    "                probs_array += [np.argmax(prob, axis=0)]\n",
    "                    \n",
    "            oms = np.array(probs_array)\n",
    "            \n",
    "            # oms = np.argmax(outs.squeeze(), axis=1)            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "\n",
    "            oms = np.array(temp_mask)\n",
    "            oms = np.around(oms.reshape([oms.shape[0], size*size])).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "            \n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "model = model.to(device)\n",
    "file_names, preds = test(models, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "submission.to_csv(\"./submission/0505_EfficientFPNB4_5FOLD_FLIP_CRF2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Scale TTA (ttach package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttach as tta\n",
    "\n",
    "transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.VerticalFlip(),\n",
    "        tta.Scale(scales=[0.75, 1, 1.25]), \n",
    "        tta.Multiply(factors=[0.9, 1, 1.1]),        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for fold in range(5): \n",
    "    model_path = f'./submission{CFG.log_day}_d{CFG.dropout}_s{CFG.seed}_{CFG.model_name}_{CFG.version}_fold{fold}_best.pth'\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = Encoder(CFG.model_name, pretrained=False)\n",
    "    model.load_state_dict(checkpoint['encoder'])\n",
    "    tta_model = tta.SegmentationTTAWrapper(model, transforms)\n",
    "    models += [tta_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(test_loader):\n",
    "\n",
    "            # inference (512 x 512)\n",
    "            for n, model in enumerate(models): \n",
    "                model = model.to(device)\n",
    "                model.eval()\n",
    "                if n == 0: \n",
    "                    outs = model(torch.stack(imgs).to(device))\n",
    "                else: \n",
    "                    outs += model(torch.stack(imgs).to(device))\n",
    "\n",
    "                    \n",
    "            probs_array = []\n",
    "            for image, prob in zip(imgs, outs):\n",
    "                prob = F.softmax(prob, dim=0)                    \n",
    "                prob = dense_crf(img=np.around(invTrans(image).cpu().numpy()).astype(float), probs=prob.cpu().numpy())\n",
    "                probs_array += [np.argmax(prob, axis=0)]\n",
    "                    \n",
    "            oms = np.array(probs_array)\n",
    "            \n",
    "            # oms = np.argmax(outs.squeeze(), axis=1)            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "\n",
    "            oms = np.array(temp_mask)\n",
    "            oms = np.around(oms.reshape([oms.shape[0], size*size])).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "            \n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "model = model.to(device)\n",
    "file_names, preds = test(models, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "submission.to_csv(\"./submission/0505_EfficientFPNB4_5FOLD_FLIP_CRF3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
